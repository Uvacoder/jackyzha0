<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>CPSC340 on jzhao.xyz</title>
    <link>https://jzhao.xyz/tags/CPSC340/</link>
    <description>Recent content in CPSC340 on jzhao.xyz</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 18 Oct 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://jzhao.xyz/tags/CPSC340/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Feature Selection</title>
      <link>https://jzhao.xyz/thoughts/feature-selection/</link>
      <pubDate>Tue, 18 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>https://jzhao.xyz/thoughts/feature-selection/</guid>
      <description>Find the features (columns) of ‘X’ that are important for predicting ‘y’.
 What are the relevant factors? Which basis functions should I use among these choices?</description>
    </item>
    
    <item>
      <title>Gradient descent</title>
      <link>https://jzhao.xyz/thoughts/gradient-descent/</link>
      <pubDate>Wed, 12 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>https://jzhao.xyz/thoughts/gradient-descent/</guid>
      <description>When we minimize or maximize a function, we call it optimization.
Gradient descent is essentially an iterative optimization algorithm that takes a guess and refines it using the gradient to make a better guess.</description>
    </item>
    
    <item>
      <title>Density-based clustering</title>
      <link>https://jzhao.xyz/thoughts/density-based-clustering/</link>
      <pubDate>Wed, 28 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>https://jzhao.xyz/thoughts/density-based-clustering/</guid>
      <description>Clusters are defined by “dense” regions. Examples in non-dense regions don’t get clustered Clusters can be non-convex  It is non-parametric (there is no fixed number of clusters $k$)</description>
    </item>
    
    <item>
      <title>Hierarchical Clustering</title>
      <link>https://jzhao.xyz/thoughts/hierarchical-clustering/</link>
      <pubDate>Wed, 28 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>https://jzhao.xyz/thoughts/hierarchical-clustering/</guid>
      <description>Hierarchical clustering produces a tree of clusterings
 Each node in the tree splits the data into 2 or more clusters.</description>
    </item>
    
    <item>
      <title>K-means</title>
      <link>https://jzhao.xyz/thoughts/K-means/</link>
      <pubDate>Wed, 28 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>https://jzhao.xyz/thoughts/K-means/</guid>
      <description>Assumption that we know how many clusters there are as a prior ($k$ in K-Means). Designed for vector [[thoughts/quantization|quantization]]: replacing examples with the mean of their cluster (collapsing a bunch of examples of a class down to a single example)</description>
    </item>
    
    <item>
      <title>Clustering</title>
      <link>https://jzhao.xyz/thoughts/clustering/</link>
      <pubDate>Mon, 26 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>https://jzhao.xyz/thoughts/clustering/</guid>
      <description>We want to assign examples to &amp;ldquo;groups&amp;rdquo;.
Methods  [[thoughts/K-means|K-means]] (most popular) [[thoughts/density-based clustering]] Ensemble Clustering  Like [[thoughts/Random Forest|random forest]] but for voting for clustering This is problematic because of the label switching problem &amp;ndash; we can get clustering with permuted labels on each initialisation  Don&amp;rsquo;t vote on what specific class each cluster is Instead, vote on whether points are in the same cluster (label independent) Then, come up with labels after voting     [[thoughts/hierarchical clustering]]  </description>
    </item>
    
    <item>
      <title>Unsupervised learning</title>
      <link>https://jzhao.xyz/thoughts/unsupervised-learning/</link>
      <pubDate>Mon, 26 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>https://jzhao.xyz/thoughts/unsupervised-learning/</guid>
      <description>In [[thoughts/supervised learning|supervised learning]], we have features $x_i$ and class labels $y_i$. Write a program that produces $y_i$ form $x_i$</description>
    </item>
    
    <item>
      <title>Curse of Dimensionality</title>
      <link>https://jzhao.xyz/thoughts/Curse-of-Dimensionality/</link>
      <pubDate>Fri, 23 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>https://jzhao.xyz/thoughts/Curse-of-Dimensionality/</guid>
      <description>Volume grows exponentially with dimension
Example If want every location on to have a “neighbor” with distance $\epsilon$,
 In 1D, we need $O(1 / \epsilon)$ points In 2D, we need $O(1 / \epsilon^2)$ points In DD, we need $O(1 / \epsilon^3)$ points  Our nearest neighbour in high-dimensions might be really really far away</description>
    </item>
    
    <item>
      <title>Ensemble method</title>
      <link>https://jzhao.xyz/thoughts/Ensemble-method/</link>
      <pubDate>Fri, 23 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>https://jzhao.xyz/thoughts/Ensemble-method/</guid>
      <description>Ensemble methods are classifiers that have classifiers as input (and often have higher accuracy than regular input classifiers)
 Also called “meta-learning”.</description>
    </item>
    
    <item>
      <title>k-Nearest Neighbours (KNN)</title>
      <link>https://jzhao.xyz/thoughts/KNN/</link>
      <pubDate>Fri, 23 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>https://jzhao.xyz/thoughts/KNN/</guid>
      <description>To classify an example, we find the $k$ examples closest to the example and take the mode of the $k$ examples.</description>
    </item>
    
    <item>
      <title>Random Forest</title>
      <link>https://jzhao.xyz/thoughts/Random-Forest/</link>
      <pubDate>Fri, 23 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>https://jzhao.xyz/thoughts/Random-Forest/</guid>
      <description>Example of an [[thoughts/Ensemble method|Ensemble method]]. They are non-parametric
They work by taking a vote from a set of deep [[thoughts/decision tree|decision trees]].</description>
    </item>
    
    <item>
      <title>Naive Bayes</title>
      <link>https://jzhao.xyz/thoughts/Naive-Bayes/</link>
      <pubDate>Mon, 19 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>https://jzhao.xyz/thoughts/Naive-Bayes/</guid>
      <description>An example of a [[thoughts/probability|probabilistic]] classifier. Commonly used in spam filters (classifies as spam if the probability of spam is higher than not spam)</description>
    </item>
    
    <item>
      <title>No Free Lunch Theorem</title>
      <link>https://jzhao.xyz/thoughts/No-Free-Lunch-Theorem/</link>
      <pubDate>Mon, 19 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>https://jzhao.xyz/thoughts/No-Free-Lunch-Theorem/</guid>
      <description>All optimization algorithms perform equally well when their performance is averaged across all possible problems.
 There is no &amp;ldquo;best&amp;rdquo; machine learning model.</description>
    </item>
    
    <item>
      <title>Decision Tree</title>
      <link>https://jzhao.xyz/thoughts/decision-tree/</link>
      <pubDate>Mon, 12 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>https://jzhao.xyz/thoughts/decision-tree/</guid>
      <description>A simple program consisting of if-else decisions (decision stumps) based on the features.
 We can create a bunch of decision stumps and define a &amp;ldquo;score&amp;rdquo; for each possible rule.</description>
    </item>
    
    <item>
      <title>Linear Algebra</title>
      <link>https://jzhao.xyz/thoughts/linear-algebra/</link>
      <pubDate>Sun, 11 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>https://jzhao.xyz/thoughts/linear-algebra/</guid>
      <description>A lot of content summarized from Mark Schmidt&amp;rsquo;s notes on Linear Algebra
Notation Generally column major
 Scalar (1,1): $\alpha$ Column Vector (m, 1): $\begin{bmatrix}a_1 \ a_2 \end{bmatrix}$ Row Vector (1, n): $\begin{bmatrix}a_1 &amp;amp; a_2\end{bmatrix}$ Matrix (m, n): $\begin{bmatrix}a_{1,1} &amp;amp; a_{2,1} \ a_{1,2} &amp;amp; a_{2,2}\end{bmatrix}$  Operations Transpose $(A^T){ij} = (A){ji}$</description>
    </item>
    
    <item>
      <title>Exploratory data analysis (EDA)</title>
      <link>https://jzhao.xyz/thoughts/exploratory-data-analysis/</link>
      <pubDate>Fri, 09 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>https://jzhao.xyz/thoughts/exploratory-data-analysis/</guid>
      <description>How do you &amp;ldquo;look&amp;rdquo; at features and high-dimensional examples?
 Summary statistics  Categorical Features  Frequencies Mode Quantiles   Numerical Features  Location  Mean Median Quantiles   Spread  Range Variance Interquartile ranges     Entropy: measured &amp;ldquo;randomness&amp;rdquo; of a set of variables where entropy is $- \Sigma_{c=1}^k p_c \log p_c$ and $p_c$ is the proportion of times you have value $c$, range from $[0, \log k]$  Low entropy means it is very predictable whereas high entropy means it is very unpredictable (roughly, spread) Normal distribution has the highest entropy    Not always representative!</description>
    </item>
    
    <item>
      <title>Supervised learning</title>
      <link>https://jzhao.xyz/thoughts/supervised-learning/</link>
      <pubDate>Fri, 09 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>https://jzhao.xyz/thoughts/supervised-learning/</guid>
      <description>Input: take features of examples and corresponding labels as inputs Output: a model that can accurately predict the labels of new examples  Generally, the most successful machine learning technique (with the exception of games)</description>
    </item>
    
  </channel>
</rss>
