<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>CPSC340 on jzhao.xyz</title>
    <link>https://jzhao.xyz/tags/CPSC340/</link>
    <description>Recent content in CPSC340 on jzhao.xyz</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 23 Sep 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://jzhao.xyz/tags/CPSC340/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Curse of Dimensionality</title>
      <link>https://jzhao.xyz/thoughts/Curse-of-Dimensionality/</link>
      <pubDate>Fri, 23 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>https://jzhao.xyz/thoughts/Curse-of-Dimensionality/</guid>
      <description>Volume grows exponentially with dimension
Example If want every location on to have a “neighbor” with distance $\epsilon$,
 In 1D, we need $O(1 / \epsilon)$ points In 2D, we need $O(1 / \epsilon^2)$ points In DD, we need $O(1 / \epsilon^3)$ points  Our nearest neighbour in high-dimensions might be really really far away</description>
    </item>
    
    <item>
      <title>Ensemble method</title>
      <link>https://jzhao.xyz/thoughts/Ensemble-method/</link>
      <pubDate>Fri, 23 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>https://jzhao.xyz/thoughts/Ensemble-method/</guid>
      <description>Ensemble methods are classifiers that have classifiers as input (and often have higher accuracy than regular input classifiers)
 Also called “meta-learning”.</description>
    </item>
    
    <item>
      <title>k-Nearest Neighbours (KNN)</title>
      <link>https://jzhao.xyz/thoughts/KNN/</link>
      <pubDate>Fri, 23 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>https://jzhao.xyz/thoughts/KNN/</guid>
      <description>To classify an example, we find the $k$ examples closest to the example and take the mode of the $k$ examples.</description>
    </item>
    
    <item>
      <title>Random Forest</title>
      <link>https://jzhao.xyz/thoughts/Random-Forest/</link>
      <pubDate>Fri, 23 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>https://jzhao.xyz/thoughts/Random-Forest/</guid>
      <description>Example of an [[thoughts/Ensemble method|Ensemble method]]
They work by taking a vote from a set of deep [[thoughts/decision tree|decision trees]]. Two key ingredients to help ensure the deep decision trees make independent errors</description>
    </item>
    
    <item>
      <title>Naive Bayes</title>
      <link>https://jzhao.xyz/thoughts/Naive-Bayes/</link>
      <pubDate>Mon, 19 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>https://jzhao.xyz/thoughts/Naive-Bayes/</guid>
      <description>An example of a [[thoughts/probability|probabilistic]] classifier. Commonly used in spam filters (classifies as spam if the probability of spam is higher than not spam)</description>
    </item>
    
    <item>
      <title>No Free Lunch Theorem</title>
      <link>https://jzhao.xyz/thoughts/No-Free-Lunch-Theorem/</link>
      <pubDate>Mon, 19 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>https://jzhao.xyz/thoughts/No-Free-Lunch-Theorem/</guid>
      <description>All optimization algorithms perform equally well when their performance is averaged across all possible problems.
 There is no &amp;ldquo;best&amp;rdquo; machine learning model.</description>
    </item>
    
    <item>
      <title>Decision Tree</title>
      <link>https://jzhao.xyz/thoughts/decision-tree/</link>
      <pubDate>Mon, 12 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>https://jzhao.xyz/thoughts/decision-tree/</guid>
      <description>A simple program consisting of if-else decisions (decision stumps) based on the features.
 We can create a bunch of decision stumps and define a &amp;ldquo;score&amp;rdquo; for each possible rule.</description>
    </item>
    
    <item>
      <title>Linear Algebra</title>
      <link>https://jzhao.xyz/thoughts/linear-algebra/</link>
      <pubDate>Sun, 11 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>https://jzhao.xyz/thoughts/linear-algebra/</guid>
      <description>A lot of content summarized from Mark Schmidt&amp;rsquo;s notes on Linear Algebra
Notation Generally column major
 Scalar (1,1): $\alpha$ Column Vector (m, 1): $\begin{bmatrix}a_1 \ a_2 \end{bmatrix}$ Row Vector (1, n): $\begin{bmatrix}a_1 &amp;amp; a_2\end{bmatrix}$ Matrix (m, n): $\begin{bmatrix}a_{1,1} &amp;amp; a_{2,1} \ a_{1,2} &amp;amp; a_{2,2}\end{bmatrix}$  Operations Transpose $(A^T){ij} = (A){ji}$</description>
    </item>
    
    <item>
      <title>Exploratory data analysis (EDA)</title>
      <link>https://jzhao.xyz/thoughts/exploratory-data-analysis/</link>
      <pubDate>Fri, 09 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>https://jzhao.xyz/thoughts/exploratory-data-analysis/</guid>
      <description>How do you &amp;ldquo;look&amp;rdquo; at features and high-dimensional examples?
 Summary statistics  Categorical Features  Frequencies Mode Quantiles   Numerical Features  Location  Mean Median Quantiles   Spread  Range Variance Interquartile ranges     Entropy: measured &amp;ldquo;randomness&amp;rdquo; of a set of variables where entropy is $- \Sigma_{c=1}^k p_c \log p_c$ and $p_c$ is the proportion of times you have value $c$, range from $[0, \log k]$  Low entropy means it is very predictable whereas high entropy means it is very unpredictable (roughly, spread) Normal distribution has the highest entropy    Not always representative!</description>
    </item>
    
    <item>
      <title>Supervised learning</title>
      <link>https://jzhao.xyz/thoughts/supervised-learning/</link>
      <pubDate>Fri, 09 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>https://jzhao.xyz/thoughts/supervised-learning/</guid>
      <description>Input: take features of examples and corresponding labels as inputs Output: a model that can accurately predict the labels of new examples  Generally, the most successful machine learning technique (with the exception of games)</description>
    </item>
    
  </channel>
</rss>
